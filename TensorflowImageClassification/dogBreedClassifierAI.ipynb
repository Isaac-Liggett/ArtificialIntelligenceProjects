{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dafc3d6e",
   "metadata": {},
   "source": [
    "# Dog Breed Classifier Network Using Tensorflow\n",
    "\n",
    "This is a demonstration of my learnings in artificial intelligence specifically deep learning. Following on from my project about building a supervised deep learning model from scratch, I now have further insight into the processes of a machine learning system whereby I belive it is fitting to now move on to a more efficient and conventient library for training models.\n",
    "\n",
    "This model will be used to differentiate and classify 120 different breeds of dogs using a dataset found from http://vision.stanford.edu/aditya86/ImageNetDogs/ which has about 20,000 images. The breeds classified are:\n",
    "\n",
    "|                                |                          |                             |                    |                             |                           |                             |\n",
    "| ------------------------------ | ------------------------ | --------------------------- | ------------------ | --------------------------- | ------------------------- | --------------------------- |\n",
    "| Afghan_hound                   | Bouvier_des_Flandres     | Eskimo_dog                  | Irish_wolfhound    | Norwegian_elkhound          | Scottish_deerhound        | West_Highland_white_terrier |\n",
    "| African_hunting_dog            | Brabancon_griffon        | French_bulldog              | Italian_greyhound  | Norwich_terrier             | Sealyham_terrier          | Yorkshire_terrier           |\n",
    "| Airedale                       | Brittany_spaniel         | German_shepherd             | Japanese_spaniel   | Old_English_sheepdog        | Shetland_sheepdog         | affenpinscher               |\n",
    "| American_Staffordshire_terrier | Cardigan                 | German_short-haired_pointer | Kerry_blue_terrier | Pekinese                    | Shih-Tzu                  | basenji                     |\n",
    "| Appenzeller                    | Chesapeake_Bay_retriever | Gordon_setter               | Labrador_retriever | Pembroke                    | Siberian_husky            | basset                      |\n",
    "| Australian_terrier             | Chihuahua                | Great_Dane                  | Lakeland_terrier   | Pomeranian                  | Staffordshire_bullterrier | beagle                      |\n",
    "| Bedlington_terrier             | Dandie_Dinmont           | Great_Pyrenees              | Leonberg           | Rhodesian_ridgeback         | Sussex_spaniel            | black-and-tan_coonhound     |\n",
    "| Bernese_mountain_dog           | Doberman                 | Greater_Swiss_Mountain_dog  | Lhasa              | Rottweiler                  | Tibetan_mastiff           | bloodhound                  |\n",
    "| Blenheim_spaniel               | English_foxhound         | Ibizan_hound                | Maltese_dog        | Saint_Bernard               | Tibetan_terrier           | bluetick                    |\n",
    "| Border_collie                  | English_setter           | Irish_setter                | Mexican_hairless   | Saluki                      | Walker_hound              | borzoi                      |\n",
    "| Border_terrier                 | English_springer         | Irish_terrier               | Newfoundland       | Samoyed                     | Weimaraner                | boxer                       |\n",
    "| Boston_bull                    | EntleBucher              | Irish_water_spaniel         | Norfolk_terrier    | Scotch_terrier              | Welsh_springer_spaniel    | briard                      |\n",
    "| pug                            | redbone                  | schipperke                  | silky_terrier      | soft-coated_wheaten_terrier | miniature_schnauzer       | standard_poodle             |\n",
    "| toy_poodle                     | toy_terrier              | vizsla                      | whippet            | wire-haired_fox_terrier     | chow                      | groenendael                 |\n",
    "| kelpie                         | komondor                 | kuvasz                      | malamute           | malinois                    | dingo                     | keeshond                    |\n",
    "| papillon                       | pinscher                 | poodle                      | bull_mastiff       | cairn                       | clumber                   | otterhound                  |\n",
    "| cocker_spaniel                 | collie                   | curly-coated_retriever      | golden_retriever   | dhole                       | flat-coated_retriever     | standard_schnauzer          |\n",
    "| giant_schnauzer                |                          |                             |                    |                             |                           |                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06db8c",
   "metadata": {},
   "source": [
    "# Initial imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce098ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d71679",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f008fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not tf.test.gpu_device_name():\n",
    "    print(\"no GPU found.\")\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "tf.config.list_physical_devices('GPU') # this is the newer way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86df8c",
   "metadata": {},
   "source": [
    "# Preparing the image dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da6e510",
   "metadata": {},
   "source": [
    "The Images are all of different sizes as they appear to have been webscraped from the internet. Tensorflow has some inbuilt image manipulation 'layers' which resize normalise the images to prepare them for training. This is incorporated into the model  \n",
    "\n",
    "The images are also loaded into a tensorflow dataset object. This makes training easier because the image_dataset_from_directory function easily references and labels each image for the full dataset. Batches can also be defined to prevent exceeding memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2b370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806b4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_image = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(224, 224, interpolation=\"bilinear\", crop_to_aspect_ratio=False), #The images are resized to 224 x 224 pixels as corresponding to the training input size parameters\n",
    "    tf.keras.layers.Rescaling(scale=1./255) # The rgb pixel data from each image is scaled to between 0 and 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03de3e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20339 files belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "image_ds = tf.keras.utils.image_dataset_from_directory(\"./imagedataset/120breeds_dataset/\", shuffle=True, labels='inferred', batch_size=BATCH_SIZE)\n",
    "\n",
    "train_split = 0.8                          # The dataset is separated into testing and training data by an 80:20 split\n",
    "training_size = np.ceil(len(image_ds)*0.8)\n",
    "\n",
    "train_ds = image_ds.take(training_size) # Training dataset for the model to learn to differentiate\n",
    "test_ds = image_ds.skip(training_size) # Testing dataset for unseen images to test the accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330a0e99",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "This model will be trained using a method called Transfer Learning. Transfer Learning is where I use a preexisting model and either add to or modify it for training to solve the problem. This learning approach is optimal for the tools and constraints for this project which is low computing resources (Home Computer with old GPU - GTX 1060) and time constraints. \n",
    "\n",
    "I have gotten a model from TensorflowHub which is a database of pretrained AI models supplied by Google for transfer learning and other implementations. This specific model is a feature vector model which essentially analyses an image and extracts the vital features out of it. I have then added another layer connecting to this pretrained feature vector network which has 120 outputs corresponding to the 120 different dog breeds being classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e7dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "classifier = tf.keras.Sequential([\n",
    "    prepare_image, # This is the image manipulation layer created earlier\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/mobilenet_v1_100_224/feature_vector/5\", trainable=False, arguments=dict(batch_norm_momentum=0.997)),\n",
    "    tf.keras.layers.Dense(120, activation=\"softmax\"), # Softmax activation function maps the outputs to probabilities so that\n",
    "])                                                    # the highest probability output is the most likely classification\n",
    "\n",
    "classifier.compile(optimizer=\"SGD\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52ad3601",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checkpoint saving callback to save the model after each epoch \n",
    "checkpoint_path = \"./training_cp/checkpoint.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1) \n",
    "\n",
    "# Tensorboard callback to save data for analysis using Tensorboard API - https://www.tensorflow.org/tensorboard/get_started\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b0d15ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 4.4604 - accuracy: 0.0746\n",
      "Epoch 1: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 53s 292ms/step - loss: 4.4604 - accuracy: 0.0746 - val_loss: 3.6992 - val_accuracy: 0.1924\n",
      "Epoch 2/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 3.1618 - accuracy: 0.2971\n",
      "Epoch 2: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 47s 286ms/step - loss: 3.1618 - accuracy: 0.2971 - val_loss: 2.7792 - val_accuracy: 0.3897\n",
      "Epoch 3/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 2.4184 - accuracy: 0.4673\n",
      "Epoch 3: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 288ms/step - loss: 2.4184 - accuracy: 0.4673 - val_loss: 2.2206 - val_accuracy: 0.5123\n",
      "Epoch 4/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.9666 - accuracy: 0.5636\n",
      "Epoch 4: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 290ms/step - loss: 1.9666 - accuracy: 0.5636 - val_loss: 1.8748 - val_accuracy: 0.5786\n",
      "Epoch 5/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.6780 - accuracy: 0.6234\n",
      "Epoch 5: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 291ms/step - loss: 1.6780 - accuracy: 0.6234 - val_loss: 1.6627 - val_accuracy: 0.6212\n",
      "Epoch 6/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.4819 - accuracy: 0.6633\n",
      "Epoch 6: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 294ms/step - loss: 1.4819 - accuracy: 0.6633 - val_loss: 1.5145 - val_accuracy: 0.6491\n",
      "Epoch 7/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.3386 - accuracy: 0.6951\n",
      "Epoch 7: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 1.3386 - accuracy: 0.6951 - val_loss: 1.4026 - val_accuracy: 0.6596\n",
      "Epoch 8/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.2311 - accuracy: 0.7171\n",
      "Epoch 8: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 293ms/step - loss: 1.2311 - accuracy: 0.7171 - val_loss: 1.3118 - val_accuracy: 0.6776\n",
      "Epoch 9/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.1460 - accuracy: 0.7343\n",
      "Epoch 9: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 294ms/step - loss: 1.1460 - accuracy: 0.7343 - val_loss: 1.2316 - val_accuracy: 0.6936\n",
      "Epoch 10/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.0764 - accuracy: 0.7487\n",
      "Epoch 10: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 293ms/step - loss: 1.0764 - accuracy: 0.7487 - val_loss: 1.1871 - val_accuracy: 0.7050\n",
      "Epoch 11/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 1.0153 - accuracy: 0.7618\n",
      "Epoch 11: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 295ms/step - loss: 1.0153 - accuracy: 0.7618 - val_loss: 1.1376 - val_accuracy: 0.7169\n",
      "Epoch 12/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.9680 - accuracy: 0.7707\n",
      "Epoch 12: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 294ms/step - loss: 0.9680 - accuracy: 0.7707 - val_loss: 1.0991 - val_accuracy: 0.7263\n",
      "Epoch 13/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.7820\n",
      "Epoch 13: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 295ms/step - loss: 0.9211 - accuracy: 0.7820 - val_loss: 1.0669 - val_accuracy: 0.7301\n",
      "Epoch 14/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.8840 - accuracy: 0.7899\n",
      "Epoch 14: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 0.8840 - accuracy: 0.7899 - val_loss: 1.0493 - val_accuracy: 0.7334\n",
      "Epoch 15/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.8517 - accuracy: 0.7982\n",
      "Epoch 15: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 293ms/step - loss: 0.8517 - accuracy: 0.7982 - val_loss: 1.0062 - val_accuracy: 0.7416\n",
      "Epoch 16/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.8214 - accuracy: 0.8046\n",
      "Epoch 16: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 0.8214 - accuracy: 0.8046 - val_loss: 0.9921 - val_accuracy: 0.7431\n",
      "Epoch 17/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.7967 - accuracy: 0.8099\n",
      "Epoch 17: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 0.7967 - accuracy: 0.8099 - val_loss: 0.9672 - val_accuracy: 0.7487\n",
      "Epoch 18/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.7705 - accuracy: 0.8174\n",
      "Epoch 18: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 293ms/step - loss: 0.7705 - accuracy: 0.8174 - val_loss: 0.9472 - val_accuracy: 0.7510\n",
      "Epoch 19/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.7460 - accuracy: 0.8216\n",
      "Epoch 19: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 0.7460 - accuracy: 0.8216 - val_loss: 0.9315 - val_accuracy: 0.7545\n",
      "Epoch 20/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.7250 - accuracy: 0.8299\n",
      "Epoch 20: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 0.7250 - accuracy: 0.8299 - val_loss: 0.9164 - val_accuracy: 0.7621\n",
      "Epoch 21/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.8321\n",
      "Epoch 21: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 48s 292ms/step - loss: 0.7063 - accuracy: 0.8321 - val_loss: 0.9060 - val_accuracy: 0.7634\n",
      "Epoch 22/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6879 - accuracy: 0.8384\n",
      "Epoch 22: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 295ms/step - loss: 0.6879 - accuracy: 0.8384 - val_loss: 0.8962 - val_accuracy: 0.7621\n",
      "Epoch 23/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6719 - accuracy: 0.8423\n",
      "Epoch 23: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 50s 301ms/step - loss: 0.6719 - accuracy: 0.8423 - val_loss: 0.8880 - val_accuracy: 0.7647\n",
      "Epoch 24/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6563 - accuracy: 0.8448\n",
      "Epoch 24: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.6563 - accuracy: 0.8448 - val_loss: 0.8711 - val_accuracy: 0.7669\n",
      "Epoch 25/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6401 - accuracy: 0.8504\n",
      "Epoch 25: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.6401 - accuracy: 0.8504 - val_loss: 0.8644 - val_accuracy: 0.7672\n",
      "Epoch 26/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6280 - accuracy: 0.8542\n",
      "Epoch 26: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.6280 - accuracy: 0.8542 - val_loss: 0.8510 - val_accuracy: 0.7715\n",
      "Epoch 27/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6148 - accuracy: 0.8578\n",
      "Epoch 27: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.6148 - accuracy: 0.8578 - val_loss: 0.8486 - val_accuracy: 0.7697\n",
      "Epoch 28/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.6012 - accuracy: 0.8620\n",
      "Epoch 28: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.6012 - accuracy: 0.8620 - val_loss: 0.8278 - val_accuracy: 0.7728\n",
      "Epoch 29/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5894 - accuracy: 0.8649\n",
      "Epoch 29: saving model to ./training_cp\\checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 49s 297ms/step - loss: 0.5894 - accuracy: 0.8649 - val_loss: 0.8218 - val_accuracy: 0.7730\n",
      "Epoch 30/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5801 - accuracy: 0.8664\n",
      "Epoch 30: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 50s 300ms/step - loss: 0.5801 - accuracy: 0.8664 - val_loss: 0.8161 - val_accuracy: 0.7774\n",
      "Epoch 31/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.8685\n",
      "Epoch 31: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.5680 - accuracy: 0.8685 - val_loss: 0.8177 - val_accuracy: 0.7735\n",
      "Epoch 32/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5565 - accuracy: 0.8723\n",
      "Epoch 32: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.5565 - accuracy: 0.8723 - val_loss: 0.8022 - val_accuracy: 0.7758\n",
      "Epoch 33/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.8735\n",
      "Epoch 33: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.5478 - accuracy: 0.8735 - val_loss: 0.7966 - val_accuracy: 0.7814\n",
      "Epoch 34/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.8781\n",
      "Epoch 34: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.5371 - accuracy: 0.8781 - val_loss: 0.7880 - val_accuracy: 0.7814\n",
      "Epoch 35/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5283 - accuracy: 0.8804\n",
      "Epoch 35: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.5283 - accuracy: 0.8804 - val_loss: 0.7852 - val_accuracy: 0.7809\n",
      "Epoch 36/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5198 - accuracy: 0.8821\n",
      "Epoch 36: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.5198 - accuracy: 0.8821 - val_loss: 0.7880 - val_accuracy: 0.7784\n",
      "Epoch 37/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.8846\n",
      "Epoch 37: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.5125 - accuracy: 0.8846 - val_loss: 0.7798 - val_accuracy: 0.7832\n",
      "Epoch 38/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.8877\n",
      "Epoch 38: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.5018 - accuracy: 0.8877 - val_loss: 0.7717 - val_accuracy: 0.7829\n",
      "Epoch 39/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8888\n",
      "Epoch 39: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4947 - accuracy: 0.8888 - val_loss: 0.7726 - val_accuracy: 0.7827\n",
      "Epoch 40/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.8918\n",
      "Epoch 40: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4853 - accuracy: 0.8918 - val_loss: 0.7690 - val_accuracy: 0.7829\n",
      "Epoch 41/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.8943\n",
      "Epoch 41: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4788 - accuracy: 0.8943 - val_loss: 0.7573 - val_accuracy: 0.7840\n",
      "Epoch 42/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4739 - accuracy: 0.8954\n",
      "Epoch 42: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4739 - accuracy: 0.8954 - val_loss: 0.7595 - val_accuracy: 0.7822\n",
      "Epoch 43/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4662 - accuracy: 0.8979\n",
      "Epoch 43: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.4662 - accuracy: 0.8979 - val_loss: 0.7524 - val_accuracy: 0.7880\n",
      "Epoch 44/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.8995\n",
      "Epoch 44: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4611 - accuracy: 0.8995 - val_loss: 0.7523 - val_accuracy: 0.7865\n",
      "Epoch 45/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4543 - accuracy: 0.9025\n",
      "Epoch 45: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4543 - accuracy: 0.9025 - val_loss: 0.7465 - val_accuracy: 0.7865\n",
      "Epoch 46/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.9054\n",
      "Epoch 46: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4462 - accuracy: 0.9054 - val_loss: 0.7398 - val_accuracy: 0.7906\n",
      "Epoch 47/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4396 - accuracy: 0.9065\n",
      "Epoch 47: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.4396 - accuracy: 0.9065 - val_loss: 0.7415 - val_accuracy: 0.7906\n",
      "Epoch 48/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4356 - accuracy: 0.9085\n",
      "Epoch 48: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4356 - accuracy: 0.9085 - val_loss: 0.7419 - val_accuracy: 0.7908\n",
      "Epoch 49/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.9114\n",
      "Epoch 49: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.4293 - accuracy: 0.9114 - val_loss: 0.7331 - val_accuracy: 0.7913\n",
      "Epoch 50/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.9110\n",
      "Epoch 50: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4242 - accuracy: 0.9110 - val_loss: 0.7273 - val_accuracy: 0.7906\n",
      "Epoch 51/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.9131\n",
      "Epoch 51: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4190 - accuracy: 0.9131 - val_loss: 0.7256 - val_accuracy: 0.7926\n",
      "Epoch 52/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.9149\n",
      "Epoch 52: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.4133 - accuracy: 0.9149 - val_loss: 0.7242 - val_accuracy: 0.7913\n",
      "Epoch 53/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4072 - accuracy: 0.9170\n",
      "Epoch 53: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.4072 - accuracy: 0.9170 - val_loss: 0.7161 - val_accuracy: 0.7974\n",
      "Epoch 54/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.4012 - accuracy: 0.9201\n",
      "Epoch 54: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.4012 - accuracy: 0.9201 - val_loss: 0.7225 - val_accuracy: 0.7893\n",
      "Epoch 55/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3970 - accuracy: 0.9215\n",
      "Epoch 55: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.3970 - accuracy: 0.9215 - val_loss: 0.7146 - val_accuracy: 0.7936\n",
      "Epoch 56/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.9227\n",
      "Epoch 56: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3926 - accuracy: 0.9227 - val_loss: 0.7126 - val_accuracy: 0.7977\n",
      "Epoch 57/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3876 - accuracy: 0.9235\n",
      "Epoch 57: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3876 - accuracy: 0.9235 - val_loss: 0.7110 - val_accuracy: 0.7936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.9250\n",
      "Epoch 58: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3845 - accuracy: 0.9250 - val_loss: 0.7083 - val_accuracy: 0.7944\n",
      "Epoch 59/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.9265\n",
      "Epoch 59: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3787 - accuracy: 0.9265 - val_loss: 0.7060 - val_accuracy: 0.7994\n",
      "Epoch 60/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.9272\n",
      "Epoch 60: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.3755 - accuracy: 0.9272 - val_loss: 0.7080 - val_accuracy: 0.7913\n",
      "Epoch 61/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.9281\n",
      "Epoch 61: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3703 - accuracy: 0.9281 - val_loss: 0.7054 - val_accuracy: 0.7982\n",
      "Epoch 62/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3667 - accuracy: 0.9307\n",
      "Epoch 62: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 50s 299ms/step - loss: 0.3667 - accuracy: 0.9307 - val_loss: 0.6991 - val_accuracy: 0.7977\n",
      "Epoch 63/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3622 - accuracy: 0.9311\n",
      "Epoch 63: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3622 - accuracy: 0.9311 - val_loss: 0.7089 - val_accuracy: 0.7951\n",
      "Epoch 64/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.9334\n",
      "Epoch 64: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3577 - accuracy: 0.9334 - val_loss: 0.7030 - val_accuracy: 0.7956\n",
      "Epoch 65/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3553 - accuracy: 0.9331\n",
      "Epoch 65: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3553 - accuracy: 0.9331 - val_loss: 0.7005 - val_accuracy: 0.7959\n",
      "Epoch 66/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3512 - accuracy: 0.9334\n",
      "Epoch 66: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3512 - accuracy: 0.9334 - val_loss: 0.6902 - val_accuracy: 0.8002\n",
      "Epoch 67/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3467 - accuracy: 0.9356\n",
      "Epoch 67: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3467 - accuracy: 0.9356 - val_loss: 0.6887 - val_accuracy: 0.8012\n",
      "Epoch 68/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3430 - accuracy: 0.9363\n",
      "Epoch 68: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3430 - accuracy: 0.9363 - val_loss: 0.6957 - val_accuracy: 0.7989\n",
      "Epoch 69/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3397 - accuracy: 0.9374\n",
      "Epoch 69: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3397 - accuracy: 0.9374 - val_loss: 0.6950 - val_accuracy: 0.7961\n",
      "Epoch 70/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.9393\n",
      "Epoch 70: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3354 - accuracy: 0.9393 - val_loss: 0.6912 - val_accuracy: 0.7999\n",
      "Epoch 71/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.9413\n",
      "Epoch 71: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3323 - accuracy: 0.9413 - val_loss: 0.6864 - val_accuracy: 0.7982\n",
      "Epoch 72/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3290 - accuracy: 0.9418\n",
      "Epoch 72: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3290 - accuracy: 0.9418 - val_loss: 0.6891 - val_accuracy: 0.7994\n",
      "Epoch 73/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.9424\n",
      "Epoch 73: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.3271 - accuracy: 0.9424 - val_loss: 0.6794 - val_accuracy: 0.8027\n",
      "Epoch 74/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.9438\n",
      "Epoch 74: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3227 - accuracy: 0.9438 - val_loss: 0.6807 - val_accuracy: 0.8010\n",
      "Epoch 75/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3209 - accuracy: 0.9435\n",
      "Epoch 75: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3209 - accuracy: 0.9435 - val_loss: 0.6836 - val_accuracy: 0.8005\n",
      "Epoch 76/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.9457\n",
      "Epoch 76: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3176 - accuracy: 0.9457 - val_loss: 0.6803 - val_accuracy: 0.8020\n",
      "Epoch 77/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9465\n",
      "Epoch 77: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3127 - accuracy: 0.9465 - val_loss: 0.6795 - val_accuracy: 0.8010\n",
      "Epoch 78/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3105 - accuracy: 0.9481\n",
      "Epoch 78: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.3105 - accuracy: 0.9481 - val_loss: 0.6779 - val_accuracy: 0.8020\n",
      "Epoch 79/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3084 - accuracy: 0.9479\n",
      "Epoch 79: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.3084 - accuracy: 0.9479 - val_loss: 0.6805 - val_accuracy: 0.8010\n",
      "Epoch 80/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.9474\n",
      "Epoch 80: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.3065 - accuracy: 0.9474 - val_loss: 0.6815 - val_accuracy: 0.8035\n",
      "Epoch 81/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.3023 - accuracy: 0.9498\n",
      "Epoch 81: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.3023 - accuracy: 0.9498 - val_loss: 0.6737 - val_accuracy: 0.8027\n",
      "Epoch 82/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.9507\n",
      "Epoch 82: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2984 - accuracy: 0.9507 - val_loss: 0.6731 - val_accuracy: 0.8015\n",
      "Epoch 83/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.9510\n",
      "Epoch 83: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.2968 - accuracy: 0.9510 - val_loss: 0.6730 - val_accuracy: 0.8035\n",
      "Epoch 84/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9524\n",
      "Epoch 84: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.2943 - accuracy: 0.9524 - val_loss: 0.6799 - val_accuracy: 0.7982\n",
      "Epoch 85/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2912 - accuracy: 0.9526\n",
      "Epoch 85: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.2912 - accuracy: 0.9526 - val_loss: 0.6708 - val_accuracy: 0.8022\n",
      "Epoch 86/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.9535\n",
      "Epoch 86: saving model to ./training_cp\\checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2883 - accuracy: 0.9535 - val_loss: 0.6714 - val_accuracy: 0.8020\n",
      "Epoch 87/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9554\n",
      "Epoch 87: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 50s 301ms/step - loss: 0.2854 - accuracy: 0.9554 - val_loss: 0.6703 - val_accuracy: 0.8048\n",
      "Epoch 88/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.9552\n",
      "Epoch 88: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.2846 - accuracy: 0.9552 - val_loss: 0.6714 - val_accuracy: 0.8030\n",
      "Epoch 89/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9559\n",
      "Epoch 89: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2793 - accuracy: 0.9559 - val_loss: 0.6668 - val_accuracy: 0.8025\n",
      "Epoch 90/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.9563\n",
      "Epoch 90: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2792 - accuracy: 0.9563 - val_loss: 0.6680 - val_accuracy: 0.8030\n",
      "Epoch 91/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.9576\n",
      "Epoch 91: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2754 - accuracy: 0.9576 - val_loss: 0.6716 - val_accuracy: 0.7994\n",
      "Epoch 92/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2727 - accuracy: 0.9581\n",
      "Epoch 92: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.2727 - accuracy: 0.9581 - val_loss: 0.6636 - val_accuracy: 0.8030\n",
      "Epoch 93/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.9587\n",
      "Epoch 93: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2707 - accuracy: 0.9587 - val_loss: 0.6672 - val_accuracy: 0.8050\n",
      "Epoch 94/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9598\n",
      "Epoch 94: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 299ms/step - loss: 0.2682 - accuracy: 0.9598 - val_loss: 0.6644 - val_accuracy: 0.8048\n",
      "Epoch 95/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2668 - accuracy: 0.9601\n",
      "Epoch 95: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2668 - accuracy: 0.9601 - val_loss: 0.6648 - val_accuracy: 0.8050\n",
      "Epoch 96/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2635 - accuracy: 0.9607\n",
      "Epoch 96: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2635 - accuracy: 0.9607 - val_loss: 0.6584 - val_accuracy: 0.8060\n",
      "Epoch 97/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9611\n",
      "Epoch 97: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 298ms/step - loss: 0.2624 - accuracy: 0.9611 - val_loss: 0.6612 - val_accuracy: 0.8048\n",
      "Epoch 98/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9618\n",
      "Epoch 98: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.2592 - accuracy: 0.9618 - val_loss: 0.6618 - val_accuracy: 0.8060\n",
      "Epoch 99/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9622\n",
      "Epoch 99: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 296ms/step - loss: 0.2573 - accuracy: 0.9622 - val_loss: 0.6642 - val_accuracy: 0.8048\n",
      "Epoch 100/100\n",
      "164/164 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.9630\n",
      "Epoch 100: saving model to ./training_cp\\checkpoint.ckpt\n",
      "164/164 [==============================] - 49s 297ms/step - loss: 0.2557 - accuracy: 0.9630 - val_loss: 0.6576 - val_accuracy: 0.8065\n"
     ]
    }
   ],
   "source": [
    "fitdata = classifier.fit(train_ds, epochs=100, batch_size=BATCH_SIZE, callbacks=[cp_callback, tensorboard_callback], validation_data=test_ds, validation_batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25024246",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12672), started 3:51:16 ago. (Use '!kill 12672' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4afaa7366f3aeeef\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4afaa7366f3aeeef\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e83557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babdcf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABPhklEQVR4nO3dd3hUVf4G8HcmyaRNCimkQAKREoqASDOAggtIcRFWFEtWiqg/EGkKKrK6KLKwIq6NZdVVEBsr0pQeukaEUKWEQDSEAIEIpJdJMnN+fxzmTgZSJmRm7iS8n+c5z9zM3Mx8c0XyctrVCCEEiIiIiFyQVu0CiIiIiKrCoEJEREQui0GFiIiIXBaDChEREbksBhUiIiJyWQwqRERE5LIYVIiIiMhluatdQF2YTCZcuHABfn5+0Gg0apdDRERENhBCID8/H5GRkdBqq+8zqddB5cKFC4iKilK7DCIiIroJGRkZaNq0abXn1Oug4ufnB0D+oP7+/ipXQ0RERLbIy8tDVFSU8nu8OvU6qJiHe/z9/RlUiIiI6hlbpm1wMi0RERG5LAYVIiIiclkMKkREROSy6vUcFSKihsRkMqG0tFTtMojqzMPDA25ubnZ5LwYVIiIXUFpairS0NJhMJrVLIbKLwMBAhIeH13mfMwYVIiKVCSGQmZkJNzc3REVF1bgBFpErE0KgqKgIWVlZAICIiIg6vR+DChGRysrLy1FUVITIyEj4+PioXQ5RnXl7ewMAsrKy0Lhx4zoNAzG2ExGpzGg0AgB0Op3KlRDZjzl0l5WV1el9GFSIiFwE71lGDYm9/jwzqBAREZHLYlAhIiIil8WgQkRELq9v376YOnWqzecvXboUgYGBDqvnejt37oRGo0FOTo7N3zNmzBgMHz7cYTWZ1fbauRqu+qlMcTHwxx+AmxvQpIna1RARkZ2dOXMGMTExOHToEO644446v1/Pnj2RmZmJgIAAm7/nvffegxCizp/d0LFHpTLffQc0awaMG6d2JUREpCJbdwrW6XS13twsICDAqb0+9RWDSmV8feVjQYG6dRDRrUkIoLBQnVaLf+H37dsXkyZNwtSpU9GoUSOEhYXhk08+QWFhIcaOHQs/Pz+0bNkSGzdutPq+Xbt2oXv37vD09ERERARefvlllJeXK68XFhZi1KhR0Ov1iIiIwMKFC2/4bIPBgOnTp6NJkybw9fVFjx49sHPnTptrj4mJAQB07twZGo0Gffv2BWAZjpk7dy4iIyMRGxsLAPjiiy/QtWtX+Pn5ITw8HI8//riyoRlw49CPeehp8+bNaNu2LfR6PQYNGoTMzEzle64f+unbty8mT56MF198EUFBQQgPD8fs2bOt6j558iR69+4NLy8vtGvXDlu3boVGo8GaNWts/tmzs7MxatQoNGrUCD4+Phg8eDBOnz6tvJ6eno6hQ4eiUaNG8PX1Rfv27bFhwwble+Pj4xEaGgpvb2+0atUKS5YssfmzbwaDSmXMQaWwUN06iOjWVFQE6PXqtKKiWpX6+eefIyQkBPv27cOkSZMwYcIEPPzww+jZsycOHjyI++67D0888QSKrr3v+fPnMWTIEHTr1g1HjhzB4sWL8emnn+LNN99U3nPGjBnYtWsX1q5diy1btmDnzp04ePCg1ec+99xz2LNnD5YvX45ff/0VDz/8MAYNGmT1C7c6+/btAwBs3boVmZmZWLVqlfLatm3bkJKSgoSEBKxbtw6A3Atkzpw5OHLkCNasWYMzZ85gzJgx1X5GUVER3n77bXzxxRfYvXs3zp49i+nTp9d4PX19fbF371689dZbeOONN5CQkABA7rczfPhw+Pj4YO/evfj4448xa9Ysm37eisaMGYP9+/fj+++/x549eyCEwJAhQ5T9TiZOnAiDwYDdu3fj6NGj+Oc//wm9Xg8AePXVV3HixAls3LgRycnJWLx4MUJCQmpdQ62Ieiw3N1cAELm5ufZ94x9/FAIQolUr+74vEVEliouLxYkTJ0RxcbF8oqBA/h2kRisosLnuPn36iN69eytfl5eXC19fX/HEE08oz2VmZgoAYs+ePUIIIV555RURGxsrTCaTcs6iRYuEXq8XRqNR5OfnC51OJ7799lvl9StXrghvb28xZcoUIYQQ6enpws3NTZw/f96qnn79+omZM2cKIYRYsmSJCAgIqLL2tLQ0AUAcOnTI6vnRo0eLsLAwYTAYqv3Zk5KSBACRn58vhBBix44dAoDIzs5WPh+ASE1Ntfo5w8LCrD5r2LBhytfXX08hhOjWrZt46aWXhBBCbNy4Ubi7u4vMzEzl9YSEBAFArF69uspa+/Tpo1y7U6dOCQAiMTFRef3y5cvC29tbueYdOnQQs2fPrvS9hg4dKsaOHVvlZ1V0w5/rCmrz+5uTaStzLTmyR4WIVOHjo97Qcy238O/YsaNy7ObmhuDgYHTo0EF5LiwsDACUYZLk5GTExcVZzeXo1asXCgoKcO7cOWRnZ6O0tBQ9evRQXg8KClKGYADg6NGjMBqNaN26tVUtBoMBwcHBtaq/Mh06dLhhl+ADBw5g9uzZOHLkCLKzs5WbR549exbt2rWr9H18fHzQokUL5euIiAir4aLKVLye139PSkoKoqKiEB4errzevXt3238wyOvv7u5udX2Dg4MRGxuL5ORkAMDkyZMxYcIEbNmyBf3798eIESOUuiZMmIARI0YovWXDhw9Hz549a1VDbTGoVIZzVIhITRqN5e8hF+fh4WH1tUajsXrOHEjseVfogoICuLm54cCBAzfcQ8Y8RFEXvtdd+8LCQgwcOBADBw7EV199hdDQUJw9exYDBw6sdrJtZddG1DAHqLLvcfYdtZ966ikMHDgQ69evx5YtWzBv3jwsXLgQkyZNwuDBg5Geno4NGzYgISEB/fr1w8SJE/H22287rB7OUalMxTkqXDpGRGQ3bdu2VeZFmCUmJsLPzw9NmzZFixYt4OHhgb179yqvZ2dn49SpU8rXnTt3htFoRFZWFlq2bGnVKvY2VMfcY2K+z1J1Tp48iStXrmD+/Pm4++670aZNmxp7RhwhNjYWGRkZuHTpkvJcUlJSrd6jbdu2KC8vt7q+V65cQUpKilXPUFRUFMaPH49Vq1bhhRdewCeffKK8FhoaitGjR+PLL7/Eu+++i48//rgOP1XNGFQqYw4qRiNg49I0IiKq2bPPPouMjAxMmjQJJ0+exNq1a/H3v/8dzz//PLRaLfR6PcaNG4cZM2Zg+/btOHbsGMaMGQOt1vLrqnXr1oiPj8eoUaOwatUqpKWlYd++fZg3bx7Wr19vUx2NGzeGt7c3Nm3ahEuXLiE3N7fKc6Ojo6HT6fDBBx/g999/x/fff485c+bU+VrU1oABA9CiRQuMHj0av/76KxITE/G3v/0NgO331WnVqhWGDRuGp59+Gj/99BOOHDmCv/71r2jSpAmGDRsGAJg6dSo2b96MtLQ0HDx4EDt27EDbtm0BAK+99hrWrl2L1NRUHD9+HOvWrVNecxQGlcpU7PbjPBUiIrtp0qQJNmzYgH379qFTp04YP348xo0bp/zCBYAFCxbg7rvvxtChQ9G/f3/07t0bXbp0sXqfJUuWYNSoUXjhhRcQGxuL4cOHIykpCdHR0TbV4e7ujvfffx8fffQRIiMjlV/SlQkNDcXSpUuxYsUKtGvXDvPnz3foUEdV3NzcsGbNGhQUFKBbt2546qmnlFU/Xl5eNr/PkiVL0KVLF/z5z39GXFwchBDYsGGDMuxkNBoxceJEtG3bFoMGDULr1q3x73//G4DsiZo5cyY6duyIe+65B25ubli+fLn9f9gKNKKmATMXlpeXh4CAAOTm5sLf39++b+7lBRgMQHo6YOMffCKim1FSUoK0tDTExMTU6hcOUWJiInr37o3U1FSribuuoLo/17X5/c3JtFXx9ZVBhT0qRETkIlavXg29Xo9WrVohNTUVU6ZMQa9evVwupNgTg0pVfH2Bq1cZVIiIyGXk5+fjpZdewtmzZxESEoL+/ftXunNvQ8KgUhXuTktERC5m1KhRGDVqlNplOBUn01aFm74RERGpjkGlKtz0jYiISHUMKlXh0A8REZHqGFSqwqBCRESkOgaVqnCOChERkeoYVKrCOSpERA7XvHlzvPvuu8rXGo0Ga9asqfL8M2fOQKPR4PDhw3X6XHu9T03GjBmD4cOHO/QzGjouT64Kh36IiJwuMzMTjRo1sut7jhkzBjk5OVYBKCoqCpmZmQgJCbHrZ5H9MahUhUGFiMjpbL37cV25ubk57bOobjj0UxXOUSEiqtLHH3+MyMhImEwmq+eHDRuGJ598EgDw22+/YdiwYQgLC4Ner0e3bt2wdevWat/3+qGfffv2oXPnzvDy8kLXrl1x6NAhq/ONRiPGjRuHmJgYeHt7IzY2Fu+9957y+uzZs/H5559j7dq10Gg00Gg02LlzZ6VDP7t27UL37t3h6emJiIgIvPzyyygvL1de79u3LyZPnowXX3wRQUFBCA8Px+zZs2t13QwGAyZPnozGjRvDy8sLvXv3RlJSkvJ6dnY24uPjERoaCm9vb7Rq1QpLliwBAJSWluK5555DREQEvLy80KxZM8ybN69Wn18fsUelKpyjQkQqEQIoKlLns318AI2m5vMefvhhTJo0CTt27EC/fv0AAFevXsWmTZuwYcMGAEBBQQGGDBmCuXPnwtPTE8uWLcPQoUORkpJi012OCwoK8Oc//xkDBgzAl19+ibS0NEyZMsXqHJPJhKZNm2LFihUIDg7Gzz//jGeeeQYREREYOXIkpk+fjuTkZOTl5Sm/8IOCgnDhwgWr9zl//jyGDBmCMWPGYNmyZTh58iSefvppeHl5WYWRzz//HM8//zz27t2LPXv2YMyYMejVqxcGDBhQ80UD8OKLL2LlypX4/PPP0axZM7z11lsYOHAgUlNTERQUhFdffRUnTpzAxo0bERISgtTUVBQXFwMA3n//fXz//ff49ttvER0djYyMDGRkZNj0ufWaqMdyc3MFAJGbm2v/N//ySyEAIfr3t/97ExFVUFxcLE6cOCGKi4uFEEIUFMi/ftRoBQW21z1s2DDx5JNPKl9/9NFHIjIyUhiNxiq/p3379uKDDz5Qvm7WrJn417/+pXwNQKxevVp5v+DgYOW6CCHE4sWLBQBx6NChKj9j4sSJYsSIEcrXo0ePFsOGDbM6Jy0tzep9XnnlFREbGytMJpNyzqJFi4Rer1d+nj59+ojevXtbvU+3bt3ESy+9VGUtFT+7oKBAeHh4iK+++kp5vbS0VERGRoq33npLCCHE0KFDxdixYyt9r0mTJok//elPVjW6suv/XFdUm9/fHPqpCueoEBFVKz4+HitXroTBYAAAfPXVV3j00Ueh1cpfLQUFBZg+fTratm2LwMBA6PV6JCcn4+zZsza9f3JyMjp27AgvLy/lubi4uBvOW7RoEbp06YLQ0FDo9Xp8/PHHNn9Gxc+Ki4uDpkJ3Uq9evVBQUIBz584pz3Xs2NHq+yIiIpCVlWXTZ/z2228oKytDr169lOc8PDzQvXt3JCcnAwAmTJiA5cuX44477sCLL76In3/+WTl3zJgxOHz4MGJjYzF58mRs2bKlVj9jfcWhn6pwjgoRqcTHR71RZx8f288dOnQohBBYv349unXrhh9//BH/+te/lNenT5+OhIQEvP3222jZsiW8vb3x0EMPobS01G71Ll++HNOnT8fChQsRFxcHPz8/LFiwAHv37rXbZ1Tk4eFh9bVGo7lhnk5dDB48GOnp6diwYQMSEhLQr18/TJw4EW+//TbuvPNOpKWlYePGjdi6dStGjhyJ/v3747vvvrPb57siBpWqcI4KEalEo7H8FeTKvLy88OCDD+Krr75CamoqYmNjceeddyqvJyYmYsyYMfjLX/4CQPawnDlzxub3b9u2Lb744guUlJQovSq//PKL1TmJiYno2bMnnn32WeW53377zeocnU4Ho9FY42etXLkSQgilVyUxMRF+fn5o2rSpzTVXp0WLFtDpdEhMTESzZs0AAGVlZUhKSsLUqVOV80JDQzF69GiMHj0ad999N2bMmIG3334bAODv749HHnkEjzzyCB566CEMGjQIV69eRVBQkF1qdEUc+qkKh36IiGoUHx+P9evX47PPPkN8fLzVa61atcKqVatw+PBhHDlyBI8//niteh8ef/xxaDQaPP300zhx4gQ2bNig/MKu+Bn79+/H5s2bcerUKbz66qtWq2gAuancr7/+ipSUFFy+fBllZWU3fNazzz6LjIwMTJo0CSdPnsTatWvx97//Hc8//7wylFVXvr6+mDBhAmbMmIFNmzbhxIkTePrpp1FUVIRx48YBAF577TWsXbsWqampOH78ONatW4e2bdsCAN555x188803OHnyJE6dOoUVK1YgPDwcgYGBdqnPVTGoVIVBhYioRn/6058QFBSElJQUPP7441avvfPOO2jUqBF69uyJoUOHYuDAgVY9LjXR6/X44YcfcPToUXTu3BmzZs3CP//5T6tz/u///g8PPvggHnnkEfTo0QNXrlyx6l0BgKeffhqxsbHo2rUrQkNDkZiYeMNnNWnSBBs2bMC+ffvQqVMnjB8/HuPGjcPf/va3WlyNms2fPx8jRozAE088gTvvvBOpqanYvHmzssmdTqfDzJkz0bFjR9xzzz1wc3PD8uXLAQB+fn5466230LVrV3Tr1g1nzpzBhg0b7BakXJVGCCHULuJm5eXlISAgALm5ufD397fvm1+8CEREyD5Yo9G29XpERDehpKQEaWlpiImJsZo4SlSfVffnuja/vxt2DKsL82RaIYBra9iJiIjIuRhUqlJx6juHf4iIiFTBoFIVrRbw9pbHDCpERESqYFCpDifUEhERqcplgsr8+fOh0Wis1pKrzjxPhXupEJET1OO1DUQ3sNefZ5cIKklJSfjoo49u2JpYdexRISIncHNzAwC77thKpLaia3fWvH4339pSfWfagoICxMfH45NPPsGbb76pdjnWGFSIyAnc3d3h4+ODP/74Ax4eHg1+Xwxq2IQQKCoqQlZWFgIDA5UgfrNUDyoTJ07E/fffj/79+9cYVAwGg3LzK0Cuw3YoBhUicgKNRoOIiAikpaUhPT1d7XKI7CIwMBDh4eF1fh9Vg8ry5ctx8ODBG7Y7rsq8efPw+uuvO7iqCjhHhYicRKfToVWrVhz+oQbBw8Ojzj0pZqoFlYyMDEyZMgUJCQk278Q4c+ZMPP/888rXeXl5iIqKclSJ7FEhIqfSarXcmZboOqoFlQMHDiArK8vqvg9GoxG7d+/Ghx9+CIPBcEMa8/T0hKenp/OKZFAhIiJSlWpBpV+/fjh69KjVc2PHjkWbNm3w0ksv2a3LqE4YVIiIiFSlWlDx8/PD7bffbvWcr68vgoODb3heNeY5KgwqREREquAauOqYe1Q4mZaIiEgVqi9Prmjnzp1ql2CNQz9ERESqYo9KdRhUiIiIVMWgUh0GFSIiIlUxqFSHG74RERGpikGlOuxRISIiUhWDSnUYVIiIiFTFoFIdBhUiIiJVMahUh3NUiIiIVMWgUh1zj0pxMWAyqVsLERHRLYhBpTrmoAIARUXq1UFERHSLYlCpjrc3oNHIY85TISIicjoGlepoNLzfDxERkYoYVGrClT9ERESqYVCpCYMKERGRahhUasKgQkREpBoGlZpwLxUiIiLVuKtdgCvKzASOHAH8/YGe7FEhIiJSDYNKJbZuBUaNAu67D9jMoEJERKQaDv1UIjBQPubkgHNUiIiIVMSgUgmroMI5KkRERKphUKlEQIB8ZI8KERGRuhhUKsGhHyIiItfAoFIJc1ApLQVKdP7yCwYVIiIip2NQqYReD2ivXZkcbZA84BwVIiIip2NQqYRWW2GeiqaRPGCPChERkdMxqFRBmadi4tAPERGRWhhUqsCgQkREpD4GlSooQaX82j4qDCpEREROx6BSBWWOStm15cmcTEtEROR0DCpVUHpUDN7ygD0qRERETsegUgVzUMk1eMoDBhUiIiKnY1CpgtKjUnQtqBgMQHm5avUQERHdihhUqqAElUIPy5PsVSEiInIqBpUqKEElXwu4uckvGFSIiIicikGlCpYbE2p4Y0IiIiKVMKhUQVmenAMGFSIiIpUwqFTB0qMCeZdCgHupEBERORmDShWsggp7VIiIiFTBoFIFc1ApKQEM3te+YFAhIiJyKgaVKvj7AxqNPM7VhcoDBhUiIiKnYlCpglYrwwoA5OgaywPOUSEiInIqBpVqKPNU3EPkAXtUiIiInIpBpRrKEmVtkDxgUCEiInIqBpVqKD0qmkbygEGFiIjIqRhUqqEEFXGta4VzVIiIiJyKQaUaNwQV9qgQERE5FYNKNcxBJdd4bWdaBhUiIiKnYlCphtKjUsadaYmIiNTAoFINJaiU+sgDzlEhIiJyKgaVaihBxeAlD9ijQkRE5FQMKtVQ9lEp9pQHDCpEREROxaBSDaVHpUgnDxhUiIiInIpBpRpKUClwlweco0JERORUDCrVUJYnF7jJA/aoEBERORWDSjXMQaWwSIsyuAPl5UBpqao1ERER3UoYVKrh7285zgV3pyUiInI2BpVquLsDfn7yOMc9VB4wqBARETkNg0oNlCXK3hHygBNqiYiInIZBpQbKyh/PMHnAHhUiIiKnYVCpgRJUdI3lAYMKERGR0zCo1EAJKh4h8oBDP0RERE7DoFIDZS8Vc4/K1auq1UJERHSrYVCpgaVH5dqqnytXVKuFiIjoVsOgUgMlqLgFy4PLl1WrhYiI6FbDoFIDZXmyJlAeMKgQERE5DYNKDZQeFeO1bWo59ENEROQ0DCo1UIJKua88YI8KERGR0zCo1EAJKgZvecCgQkRE5DQMKjVQgkqxpzzg0A8REZHTMKjUQNlHpdBdHly+DAihWj1ERES3ElWDyuLFi9GxY0f4+/vD398fcXFx2Lhxo5ol3cAcVPILtCiHG1Baym30iYiInETVoNK0aVPMnz8fBw4cwP79+/GnP/0Jw4YNw/Hjx9Usy4p5eTIA5Hle252W81SIiIicQtWgMnToUAwZMgStWrVC69atMXfuXOj1evzyyy9qlmXFwwPw8ZHHOY1i5AGDChERkVO4q12AmdFoxIoVK1BYWIi4uLhKzzEYDDAYDMrXeXl5TqktMBAoKgJy/KKAi+CEWiIiIidRfTLt0aNHodfr4enpifHjx2P16tVo165dpefOmzcPAQEBSouKinJKjcrKH31TecAeFSIiIqdQPajExsbi8OHD2Lt3LyZMmIDRo0fjxIkTlZ47c+ZM5ObmKi0jI8MpNSpBxSdCHjCoEBEROYXqQz86nQ4tW7YEAHTp0gVJSUl477338NFHH91wrqenJzw9PZ1doiWo6MLkAYd+iIiInEL1HpXrmUwmq3korkDZS8UjRB6wR4WIiMgpVO1RmTlzJgYPHozo6Gjk5+fj66+/xs6dO7F582Y1y7qB0qOiaSQP2KNCRETkFKoGlaysLIwaNQqZmZkICAhAx44dsXnzZgwYMEDNsm5g3kslB4HygD0qRERETqFqUPn000/V/HibKT0qRj95wKBCRETkFC43R8UVKUGl9NrObxz6ISIicgoGFRsoQaXESx7wxoREREROwaBiAyWoFHnIA4OBNyYkIiJyAgYVGyhBJU8LmPdx4fAPERGRwzGo2EDZRyVXA4RwLxUiIiJnYVCxgXl5cl4eYAq6FlTYo0JERORwDCo2MAcVIYC8wGj5BXtUiIiIHI5BxQZeXrIBQI7ftTs2M6gQERE5HIOKjZQJtb5N5AGHfoiIiByOQcVGSlDxCpcH7FEhIiJyOAYVGwUFycfLbmHygD0qREREDsegYqOICPmYaWosD9ijQkRE5HAMKjZqcm1qyoWSYHnAoEJERORwDCo2ioyUjxcK/eUBh36IiIgcjkHFRkpQyfGVB7wxIRERkcMxqNhICSpXdPKgpAQoKlKvICIiolsAg4qNzEHlfKYW0F0LKxz+ISIicigGFRuZg0purgaFQdydloiIyBkYVGzk7w/4+MjjTP9YecAeFSIiIodiULGRRlNhnopvK3nAHhUiIiKHYlCpBSWo6JrJAwYVIiIih2JQqQVl0zfttTkqHPohIiJyKAaVWlB6VARvTEhEROQMDCq1oASV0hB5wKBCRETkULUOKsXFxSiqsNFZeno63n33XWzZssWuhbkiJagUNZIHHPohIiJyqFoHlWHDhmHZsmUAgJycHPTo0QMLFy7EsGHDsHjxYrsX6EqUTd/y9PKAPSpEREQOVeugcvDgQdx9990AgO+++w5hYWFIT0/HsmXL8P7779u9QFei9Khc9YIA2KNCRETkYLUOKkVFRfDz8wMAbNmyBQ8++CC0Wi3uuusupKen271AVxIRIR+LStyQB3/2qBARETlYrYNKy5YtsWbNGmRkZGDz5s247777AABZWVnw9/e3e4GuxNcXCAiQxxcQCRQX88aEREREDlTroPLaa69h+vTpaN68OXr06IG4uDgAsnelc+fOdi/Q1SjDP27R8oDDP0RERA5T66Dy0EMP4ezZs9i/fz82bdqkPN+vXz/861//smtxrkjZ9E3fWh5w+IeIiMhh3G/mm8LDwxEeLjc9y8vLw/bt2xEbG4s2bdrYtThXpPSoeN0G5IJBhYiIyIFq3aMycuRIfPjhhwDknipdu3bFyJEj0bFjR6xcudLuBboaJai4c+iHiIjI0WodVHbv3q0sT169ejWEEMjJycH777+PN9980+4FuholqODaAXtUiIiIHKbWQSU3NxdBQUEAgE2bNmHEiBHw8fHB/fffj9OnT9u9QFejBJXyxvKAPSpEREQOU+ugEhUVhT179qCwsBCbNm1SlidnZ2fDy8vL7gW6GmV32hIZ1tijQkRE5Di1nkw7depUxMfHQ6/Xo1mzZujbty8AOSTUoUMHe9fncpQelQJ/CAAaBhUiIiKHqXVQefbZZ9G9e3dkZGRgwIAB0Gplp8xtt912S8xRubbYCWVGN1xBMEI49ENEROQwGiGEuNlvNn+rRqOxW0G1kZeXh4CAAOTm5jp1V9zQUDnicwQd0fEON+DQIad9NhERUX1Xm9/ftZ6jAgDLli1Dhw4d4O3tDW9vb3Ts2BFffPHFTRVbHymbviESyMxUtxgiIqIGrNZB5Z133sGECRMwZMgQfPvtt/j2228xaNAgjB8//pbYmRa4bonypUvynj9ERERkd7Weo/LBBx9g8eLFGDVqlPLcAw88gPbt22P27NmYNm2aXQt0RUpQ0cUApQDOnAHatlWzJCIiogap1j0qmZmZ6Nmz5w3P9+zZE5m3yDCIElT8rt3v58wZ1WohIiJqyGodVFq2bIlvv/32huf/97//oVWrVnYpytUpQcWjmTxgUCEiInKIWg/9vP7663jkkUewe/du9OrVCwCQmJiIbdu2VRpgGiJl0zfTtQMGFSIiIoeodY/KiBEjsHfvXoSEhGDNmjVYs2YNQkJCsG/fPvzlL39xRI0uR+lRMVzbnTYtTb1iiIiIGrBa96gAQJcuXfDll1/au5Z6wxxULub7wggt3NijQkRE5BA2BZW8vDyb39CZG6+ppXFjQKsFTCYNstAYEQwqREREDmFTUAkMDKxx91khBDQaDYxGo10Kc2Xu7nIr/QsX5F4qEX8cBAoKAL1e7dKIiIgaFJuCyo4dOxxdR70TGXktqPi2RpfCg0B6OtC+vdplERERNSg2BZU+ffo4uo56R5lQ26g9UAi58odBhYiIyK5u6l4/VCGo+F7bO4bzVIiIiOyOQeUm3bDpG5coExER2R2Dyk1SNn0zhssD9qgQERHZHYPKTVKCSnEjecCgQkREZHc3teGb2eXLl7F3714YjUZ069YNERER9qrL5d12m3xMvegHEzTQcuiHiIjI7m46qKxcuRLjxo1D69atUVZWhpSUFCxatAhjx461Z30uq0ULwMMDKCrR4iyi0fxqOpCXB9wCG94RERE5i81DPwUFBVZfv/7669i3bx/27duHQ4cOYcWKFZg1a5bdC3RV7u5AbKw8PqHvIQ/S09UriIiIqAGyOah06dIFa9euVb52d3dHVlaW8vWlS5eg0+nsW52La9dOPp4IiJMHHP4hIiKyK5uHfjZv3oyJEydi6dKlWLRoEd577z088sgjMBqNKC8vh1arxdKlSx1Yqusx7+92wr2jPOCEWiIiIruyOag0b94c69evxzfffIM+ffpg8uTJSE1NRWpqKoxGI9q0aQMvLy9H1upyzD0qxw0t5QGDChERkV3VennyY489hqSkJBw5cgR9+/aFyWTCHXfcccuFFKDC0E92OATAoEJERGRntVr1s2HDBiQnJ6NTp07473//i127diE+Ph6DBw/GG2+8AW9vb0fV6ZJatpSTagsMOpxDU0RxjgoREZFd2dyj8sILL2Ds2LFISkrC//3f/2HOnDno06cPDh48CC8vL3Tu3BkbN250ZK0uR6cDWl271c8JtGOPChERkZ3ZHFSWLl2KDRs2YPny5UhKSsIXX3wBANDpdJgzZw5WrVqFf/zjHw4r1FUpE2rRDsjJkY2IiIjswuag4uvri7RrQxsZGRk3zElp164dfvzxR/tWVw8oE2o9u8gD9qoQERHZjc1BZd68eRg1ahQiIyPRp08fzJkzx5F11RvKhFouUSYiIrI7myfTxsfHY9CgQfj999/RqlUrBAYGOrCs+kMJKqUtIABoGFSIiIjsplarfoKDgxEcHOyoWuql1q0BNzcgt8wXmYhAJIMKERGR3dR6HxV7mjdvHrp16wY/Pz80btwYw4cPR0pKipol1Zqnp1ymDFybUMslykRERHajalDZtWsXJk6ciF9++QUJCQkoKyvDfffdh8LCQjXLqjVlQi3ac44KERGRHdVq6MfeNm3aZPX10qVL0bhxYxw4cAD33HOPSlXVXrt2wOrV5r1UlgJCABqN2mURERHVe6oGlevl5uYCAIKCgip93WAwwGAwKF/n5eU5pa6aKBNq0Q7IywOys4EqfgYiIiKynapDPxWZTCZMnToVvXr1wu23317pOfPmzUNAQIDSoqKinFxl5cybvh3X3M57/hAREdmRywSViRMn4tixY1i+fHmV58ycORO5ublKy8jIcGKFVWvdGtBqgWzRCJcQxqBCRERkJy4RVJ577jmsW7cOO3bsQNOmTas8z9PTE/7+/lbNFXh7A7fdJo9PoB1w4oS6BRERETUQqgYVIQSee+45rF69Gtu3b0dMTIya5dSJ1TyVgwfVLYaIiKiBUDWoTJw4EV9++SW+/vpr+Pn54eLFi7h48SKKi4vVLOumWN2ckEGFiIjILlQNKosXL0Zubi769u2LiIgIpf3vf/9Ts6ybYtWjkp4OXLmibkFEREQNgKrLk4UQan68XSmbvmk7AibIXpUBA1StiYiIqL5zicm0DUGbNnKPt8umIPyBEODAAbVLIiIiqvcYVOzExwdo3lwec54KERGRfTCo2JF5Qu0hdGaPChERkR0wqNjR3XfLx23oB/z+u9xKn4iIiG4ag4od9e8vH3dq7kUZ3IFDh9QtiIiIqJ5jULGjO+4AQkKAAqHHL7iLwz9ERER1xKBiR1ot0K+fPN6K/pxQS0REVEcMKnZm3jolAQPYo0JERFRHDCp2Zg4q+9AduacvAXl56hZERERUjzGo2Fl0NNCqFWCEO3bgXk6oJSIiqgMGFQfg8A8REZF9MKg4gDmocEItERFR3TCoOMC99wJarcApxOLsLxfULoeIiKjeYlBxgIAAoHvnMgBAwm+3AQUFKldERERUPzGoOMiAIToAQAL6A4cPq1sMERFRPcWg4iDmeSrb0A+m/ZynQkREdDMYVBzkrrsAvc6AywjFka1/qF0OERFRvcSg4iAeHkDfTvLuyQn7A9UthoiIqJ5iUHGg/kN9AAAJlzoCly+rXA0REVH9w6DiQPc95A8A2I17cPW77SpXQ0REVP8wqDhQmzZAp8YXUApPfP3fIrXLISIiqncYVBxIowGefKwEAPDZ4c5AebnKFREREdUvDCoOFj8zGjoYcMjYCYe+OKZ2OURERPUKg4qDBYe5Y3i03Efls0XFKldDRERUvzCoOMG4R+X8lK8Ot0NJicrFEBER1SMMKk7Q7/lOiMJZZBsDsOazq2qXQ0REVG8wqDiBW1gIxjZJAAB8+iFX/xAREdmKQcVJxoyUAWVbciTOnFG3FiIiovqCQcVJYv7aC/2wFQJaLP0vlykTERHZgkHFWe64A08GrAIALPmkDCaTyvUQERHVAwwqzqLV4i/DTAhENs5meWPbNrULIiIicn0MKk7k/cAAxOMrAMCCBSoXQ0REVA8wqDjTgAF4we09uKMMCQnAjz+qXRAREZFrY1BxJn9/xNwThXH4FADw2msq10NEROTiGFSc7cEH8Qr+AZ2mFDt3Ajt2qF0QERGR62JQcbb4eER7/YGnxccAZK+KECrXRERE5KIYVJytUSPg4YfxCv4BT7cy/PQTkJCgdlFERESuiUFFDc88g0hkYoLmIwDsVSEiIqoKg4oaevUC2rbFS+VvwtujDHv3Ahs3ql0UERGR62FQUYNGAzzzDMJxCRMbfQMAePVVcLdaIiKi6zCoqOWJJwBPT7yY9QL0PkYcPAh8+qnaRREREbkWBhW1BAcDI0YgFJfxRseVAIAXXwQuXlS5LiIiIhfCoKKmZ54BAEw6+gzuvMOInBxg2jR1SyIiInIlDCpquuceoHVruBfm4pP710KrBZYvBzZtUrswIiIi18CgoqZrk2oB4M7N8zBlinz62WeBoiIV6yIiInIRDCpqGz0a0OmA/fvxxr07EBUFpKUBb7yhdmFERETqY1BRW0gIMH48AEA/5yUs+lDu/Pb228Cvv6pZGBERkfoYVFzBrFmAry+QlISh5asxYgRgNMrOFoNB7eKIiIjUw6DiCho3Bp5/Xh7PmoUP3y1HSAhw+DDwt7+pWhkREZGqGFRcxQsvyL1VTp5E+JZlyuZvb78NbN2qbmlERERqYVBxFQEBwCuvyOO//x0P3FdinrqC0aOBK1fUK42IiEgtDCqu5NlngaZNgXPngMWLsXAhEBsLXLgAPP0077BMRES3HgYVV+LlBcyeLY/nzoVPeR6+/hrw8ABWr+a9gIiI6NbDoOJqRo+W3ShXrgDz5uHOO4G5c+VLkycDK1eqWx4REZEzMai4Gnd34J//lMcLFgD79+OFF4D77weKi4GHHpI3LywvV7dMIiIiZ2BQcUXDhgEjR8rNVMaMgbbMgDVrgOnT5csLFgADBgBZWapWSURE5HAMKq5q0SK5v8rx48Drr8PdXQaUFSsAvR7YuRO4805g7161CyUiInIcBhVXFRICLF4sj//5TyApCYAc+tm3D2jTBjh/HujbF1i1Sr0yiYiIHIlBxZU9+CDw2GOAySQn2ZaUAADatpVh5f775VMPPQT8619cvkxERA0Pg4qr++ADICwMSE62LF0G4OcHrFkjt14RQu7AP3mynNZCRETUUDCouLrgYOCjj+TxggVAQoLykrs78OGHwMKFgEYjj4cPB7Kz1SmViIjI3hhU6oNhw4Ann5RDQI88Apw+rbyk0cjelBUr5H5x69YBt98ObNyoYr1ERER2wqBSXyxaBNx1l+wuGTYMyMuzennECGD3bqB1a7nl/pAhwFNP3XAaERFRvcKgUl94ecnlPU2ayPkqjz9+w4SUbt2AQ4eAadNkT8unnwIdOgCbN6tUMxERUR0xqNQnERFyBq2XF7B+PfC3v91wio8P8M47cp+V224Dzp4FBg2SK4SSk51eMRERUZ0wqNQ3Xbta7k44fz7w1VeVnnbPPcCRI3L+irs7sGGD7F157jng8mUn1ktERFQHDCr10eOPAy+/LI/HjKly5qxeL1cEnTghp7UYjXKqy223AU88ISfg5uc7r2wiIqLaYlCpr+bOlYGlvFzOpP3xxypPbdVKjhht3w7ccYcMJ19+KW8nFBICDB4MrF7ttMqJiIhsxqBSX2m1wNKlltsq//nPciZtNe69FzhwQK4OeuEFoGVLoLQU2LRJboL7zDPyrYiIiFyFqkFl9+7dGDp0KCIjI6HRaLBmzRo1y6l/PDzk+M0998h1yAMHAikp1X6LVgvcfTfw9tvAqVNyWGj6dLlK6JNPgLg4q21aiIiIVKVqUCksLESnTp2waNEiNcuo37y9gR9+kLdS/uMPYMAA4ORJm75Vo5H3DVqwQC5hDg2VE3C7dJH5h4iISG0aIVzjVnYajQarV6/G8OHDqzzHYDDAYDAoX+fl5SEqKgq5ubnw9/d3QpUu7I8/ZM/KyZNAQACwfLlcl1wLFy4Ajz5qme4SEwP06SNb375A8+Z2r5qIiG5BeXl5CAgIsOn3d72aozJv3jwEBAQoLSoqSu2SXEdoKLBrF9CrF5CbK+euvPderW6pHBkpJ9zOnCmXNKelyWkwY8fK0NKuHbB4MVBY6Lgfg4iIqCL2qDQ0BgMwYQKwZIn8+qmn5Jpkna5Wb5OfDyQmyuyzaxeQlCQXGAFAYKCceDtxIhAdbd/yiYio4WuwPSqenp7w9/e3anQdT0+5IdzChXLm7H//C/TvL4eGasHPT44czZsH/PwzcOUK8MEHcqVQTg7w1ltyP5YhQ+RSZ+7HQkREjlCvggrZyHxL5R9+APz95aST7t2BX3+96bf095e72qakyLft109uILdxo9w8LixM7suycqUceSIiIrIHBpWGbMgQ4JdfgBYtgDNngJ495c5vdaDVyi1btm6VoWX2bHnH5uJiuVLooYeA4GCgd29gzhxg794b7p1IRERkM1XnqBQUFCA1NRUA0LlzZ7zzzju49957ERQUhGgbJj/UZozrlnb1quzu2LZNfj1nDvDKKzJ12IEQcq+5r78G1q27cSuXwEC52Vz//rInpnVr2elDRES3ptr8/lY1qOzcuRP33nvvDc+PHj0aS5curfH7GVRqoaxMDgd9+KH8um9fOX+lRQu7f9SZM8CWLbJt3XrjUFBEBNC5M9Cpk6W1agW4udm9FCIickH1JqjUFYPKTfjkE2DKFDlW4+0NvPmm/NpBKaG8XG7bv22bDC2JiXLb/ut5esrN526/Xbb27WXPS/PmtV6wRERELo5Bhar3++/A00/LTVMAoEcP2bty++0O/+iiIjlMdOSIpR09Kp+vjJsb0KyZ7HHp2lVuD9O9O3tfiIjqMwYVqpkQMpxMny7vE+TmBowfL2fHhoQ4tRSTSW4ud+yYpZ04AaSmVh5gQkLkPOHBg2UvTHS0nAfDeS9ERPUDgwrZ7vx5YNIkYPVq+XVAAPDaa3ItsspjLkIAmZnyJokpKbIDaNOmypc/6/UysLRpI1cc9e4t58G4uzu/biIiqh6DCtXejh3AtGlyLAaQO7v94x9yvbELdVWUlcl5LuvWyR1z09Or3svO11cOE4WFyWNza9YMePBBedcBIiJyPgYVujlGo7y5z6xZwKVL8rkuXYD58+XaYhdVXAxkZMjQcuiQ3N8uMRHIzq76e9zd5dDRqFFyXxgvL+fVS0R0q2NQobrJzwfeeQd4+22goEA+16+fXCF0113q1mYjk0nOc0lKkkNFhYWWlpgoVyKZBQTIJdJNmsgbM5ofIyPlUuqICNkTU5HRKIemOLRERFR7DCpkH3/8IYd//v1vy5riuDhg8mRgxAjAw0Pd+uogORn44gt5n6KMjJrP9/eX840NBtmMRvnjx8UB990HDBggO5+4GomIqGYMKmRf6enA66/L3+plZfK5yEjg2WflSqHgYHXrqwOTSfa6pKUBFy7IucXmx8xMeVzV0unrNWokJ/Cae2EiImTvTMuWck8YPz/H/ixERPUFgwo5xsWLwEcfAYsXW+aw+PgA48bJXW+bN1e1PEcQQo6EZWbKUOPpaWnZ2XITu4QEuaFdXl717xUZCcTGAk2bytCi18tHHx85zyY/39J0OhlwWraUe8i0bHnj8BMRUX3FoEKOZTDIOxC+846cvQrIMY+RI4EZM2S3wi2mvBzYv1/u/ZKZaWnnzgGnTgFZWXX/jIgIGVrMwaVJEzkEVV5uaT4+ck+ZwEDZwxMaKs9zoYVbREQMKuQkQsjNTd56S97Yx+zuu+U8luHDOdv0mpwcuRdMSorsjCookD0nBQVygq+3t+xdMbfiYrl/jLldvXrznx0SInf17dZNtuBgOZxlbmVlMsw0bw5ERdXrqUdEVE8wqJDzHT4MLFgAfPut/Kc9IMc4nn1WDg01bqxqefXd1auyt+b0actjVpYMFe7usmm1Mnjk5FhaVpblP4cttFoZWho1svTCaDTy+caNLauhmjSRLTpa7ksTEMBeGyKyHYMKqefCBeA//5HNvBObm5tl05KhQ7lpiROVlAC//iqHpZKS5LLsoiI5RGRuWq0cokpPl+ffDD8/mUu1WjkyWFIiH0tL5dweo1E+mkxyCKtNG9liY+VE46ZNZQDihGOiWwODCqnPYJC9K4sWAXv3Wp4PCJBzWUaOBPr25dCQCxFC9sCkpd04MbisTL5mXhFlbunpwOXL9qtBr5eBJSBA5tmKk5fNTaeTjyEh1pONGzWSf+zOnZMtI0MGo9tuA1q0AMLD2etD5CoYVMi1pKQAy5bJjUsqbloSHAz85S9ym/4//YmTI+qpoiL5n/XcORkEzAHDy0v+J3Vzkz0tbm4yDGVkACdPyj8WJ0/KoawLF2peNVUTvd6yP2FlfHxkaAkMlHOCzK2yQOTra70yS6+37oXy9pYhqLjY0srK5Gt6vfx+vV5+VlV/rC9fljfgLCiwDKmFhnIvHro1MKiQazKZgJ07geXL5U0QK/5TPDAQeOABuZHcffdxeOgWVFBg2bsmP996CKmkRA4jmTfcMxjkannzfJ3MTMv7eHnJScHmoajffgPOnpV//JxNo5E9OVFRsgUHy3qOHbOs8K/IzU2eHxBgCTu+vrL36PbbgY4d5S7K1U35Ki+XofHsWRmg3Nxkx6Wbm+yNCgiQvU+NGql+31G6hTGokOsrLwd27wa++w5Ytcr6b229HhgyRLb77pOTGoiqUVgofzkHB8t2/RBPaakcpkpLkyGoYk+IOQyZW0mJfL+KK7PM31NxtZRWa90z4+EhzykokK24uOa6zT08mZnyfwFbw1TjxrJV7AUqL5fh5Nw5OSfIFr6+8nqFhsr3Cw0FgoLkdTD/7AUF8udr316222+X84s8PWUvUlmZvL7X7wVUUCDPCQiwNL3e0svm7l7zUJwQ8lobDJbPKi+Xz0dGyvevCyE4HKgWBhWqX4xG4OefgZUrZWi5fk/7Tp2AQYOAgQOBnj3r/rcTkRMYjbLT0DwslpEh55c3bw506AC0bSt/cZuVl8uwkplpvXS9sFDOB/r1V9lSU+Uv2OrodHJFlq+vrMPcSkvlarDc3Jrfwxm0WsucI/NwoZubDCfmn72qOjUa2WvWooVsYWHWw4xarTynYisokIH1zBn5eO6c7K1q29bSzPtWmieAG43yvTw8LM3d3RKczEHN21u+V2iobOZfSebrXlZmWZWXnS0f8/Plv8NatpSPlYWmhhqmGFSo/hJCLk/5/ntg82a5XKUiHx85Cfe++2Rr06Zh/l9MVIXCQnmvqtxc654gjcayXDw8XP5yrYrRKOcEZWfLMPXHH3KydFaWXArv7W09PycnBzh+XLZjxyrf10ertd4LSK+XdeXmypaXV/fhN3NIMJnke7syrbZ2P6+PjwwswcHyemVny5abK3/uitfW21v+VWleSWcyyaE88xBjVJTsITP38Jl7uUpLLaHVZJLhuKjIOhSbQ25UlHyMjpZ1RUfb9/owqFDDkZUl96jfvFluKnf9wH50tFz6PHiwnJDL9a1EDiUEcOWK/EWn08lmDhDV/ZtBCPmL8/rdlK+fe1RWJnuCKs7R8fa29JCY3+uPP+R8H3O7etV6Kbz5Dufm33BCyB6bZs0srWlT+VdMcrKlnTtn3StjDhwVe1DMNyWt2IqKLKGvsLDya6DVWnaODgyUP9v587KHR405VLZ64AFg7Vr7vieDCjVMQgBHj8rAsmWLnONS8Z9VHh7AXXcBPXoA3bvLx6go9rgQkVMVF8teKPOQkTnM6XSV/3VknkOVmip7Ucy3wAgMlHN7ysut5/4UF1tClLnn7MoVObxobn/8YVmFZu6J8fKyDmHu7vIcczD09ZXvbX6Ps2fl45//LDcgtycGFbo1FBUBO3YAmzYBGzfKf1ZdLzwciIsDevUCeveW9yHiUgciIlUxqNCt6fRpIDER2LdPbjL366837h/v5SV7W+6+W7aePTlcRETkZAwqRIDswzx4UIYXc7tyxfocrRa44w45ZGS+a1+bNtx1i4jIgRhUiCojhNwO9aefgB9/lC0t7cbz9Ho5RNSunaW1bSs3buB8FyKiOmNQIbLVuXOW4aKkJNkDU9WU/dBQOWxknqjbrZvcHYuIiGqFQYXoZhmN8gY0hw5Z1iueOCGn41e23edttwFdugBdu8rWsaPc9YmIiKrEoEJkbyUlwJEjcpKuebJuamrl5wYFAbGxsrVpI3fWveMOuQKJiIgYVIic4upVOVR04IDcQffAgcrnvJiFhcnA0qGDDDBt2sgwwx4YIrrFMKgQqaWoSC6TTkmRQ0jJybInJiWl6q0ng4NlYGnd2tJatgRiYiw3DCEiakAYVIhcTVGRvEnKoUNyzos5yKSnV/99QUEysMTEyFvX3nGHbM2acQUSEdVbDCpE9UVREXDqlHVLSZG77F6/50tFAQFyCCkmRt7u1dxatwaaNGGIISKXxqBC1BDk5ckel7Q0GVyOHpU9MsePyzujVcXPz7L/S5s2lluhRkXJvWDc3Z33MxARVYJBhaghKy21LJtOT5e3Xj1zRgaa33+/8bYBFWm18v7v5hYWJltMjFxqfdttsmfGy8tJPwwR3Ypq8/ub/7Qiqm90OrnkuVOnG18rLZXLpk+ckO3UKcutUM+dkz0xFy/KVhWNRoaY8HBLkAkPlz0y0dFyfkx0tLy9K4eYiMjBGFSIGhKdzjLscz2TCbh0SYaUrCx5nJUFXLhg6Y35/Xd5H/lLl2Srjp+fXJ3UsiXQqpV8bN5chpimTQFPT4f8iER0a2FQIbpVaLVARIRsVRECuHwZOH9eBhpzsMnMlL0yZ8/KlpUF5OfLOTOHDlX+XuHhcmJvcLBcvWRu4eEyyJhbaKisjYioEgwqRGSh0cjgEBpa/XnFxXJeTGqqbKdPy0dzkCkurnmIyczd3TJfxvxoHnaq+BgZCQQGcriJ6BbDoEJEteftLe8o3bbtja8JIZdWnz0rh5Wys+Uuvlevyt6azEw5X+bcORlkysvleRcu1Py5np4ysJhbkyaVN04GJmowGFSIyL40GnlbgJAQ4M47qz+3rMwyH6ayZh5+ysyUgcdgkPNpqrtVASCHmJo0kcNcwcHWzVxbxebtbb+fn4jsikGFiNTj4WGZq1KTkhIZXC5ckHNozI/Xt+JiSw/O0aO21aHXW4aeGjeWK5r8/OQtDPz95ZBTRIQMP5GR8hzOqyFyCgYVIqofvLwsO/BWRQggJ0cOK50/L3tjrly5sV2+bGllZXKlU0GB3FjPFu7uMswEBFjCTECA9XLusDDZWxMYaGl+fgw4RLXEoEJEDYdGIwNEo0byFgM1EULuAFxxufalS0BurlzVlJcn29WrcvjJHH7Ky4E//pCtNrRaOSxlHnIKDa382M8P8PEBfH3lo7l3hyGHbkEMKkR069JoZE9IQIDcC8YW5eVyCCo72xJkcnPl1+agY17xdPWq7OHJyZHza0wmS09ObZlDjrlVnGMTGirn35hDmrkFBsphLTe32n8ekYtgUCEiqg13d9vn1VRUUiIDS8Vhp8uXZa/M9ccFBUBhobxpZWGhHJ6qS8gx98gEBNw4ofj6+Th+fpbwZh7a4v2hSEX800dE5AxeXnLuSnh47b+3pMQyQfjqVeu5NhWDTk6O7NkxN4NBfn9+vmznz9987d7espmPGzWSQcccfAIDLa+bW8WAFBBgmafDvXCoFhhUiIhcnZeXZe+Y2jAY5LBUbq4cosrJsQQc82NOjvV8HPNQVm6uXEEFyKBUUiLDT115elpWV4WGygDj42PdvL1vfKwYlnx8ZFAKCpLH1KAxqBARNVQVQ8HNKCuTgaWgwBJWiovlkFR29o2Bx3yO+bz8fEtIys2VN800GCw3yrQHLy/r2zSYA0yjRjLUeHrKc8yPer2cpGx+NPf6+PlxPo+LYlAhIqLKeXhY5rLYQ2GhZbWUeeJxfr4l/Jib+euKj+ZWUiLfJztbTmwuKbHsoWMP5vBSWTMHGl9feW3c3OT8HTc3+VpEhGV4r3Fjzu2xE15FIiJyDl9f2arbC8dWQsiQY56vc/Wq9fyc7GwZbAwGGWYMBvl1YaFlsnJBgWXYq7xcvm9hoWy23KeqtnQ66x4f80Rmvd7SfH0tQ1zmZr5u5p4gHx/ZQ6TTWVoDDkUN9ycjIqKGS6OxrFSKianbewkhg0xenmXicWXN/HphoQw2RqN8LC+XQ1vmO41nZcnXrldaark9hL3pdNYrtfz95XMeHrLpdDLgmF8zt4qTpM3N09N6yMzfX4YrlTCoEBHRrU2jsfySvtn5PBUZjXLOjslk/XxxsaW3x9wDZN4V2dwKC62HuszDYebXzK2szPq9S0tvbhNCW4wcCfzvf/Z/XxsxqBAREdmTm5uc4FuZ6Gj7fIYQsifHYJCtsNB6hVd+vgwvZWWylZbKwFNxdVdenmXeT8VH83uam8o37WRQISIiqm80Gsuwjl5fdTBqAHjjCCIiInJZDCpERETkshhUiIiIyGUxqBAREZHLYlAhIiIil8WgQkRERC6LQYWIiIhcFoMKERERuSwGFSIiInJZDCpERETkshhUiIiIyGUxqBAREZHLYlAhIiIil8WgQkRERC7LXe0C6kIIAQDIy8tTuRIiIiKylfn3tvn3eHXqdVDJz88HAERFRalcCREREdVWfn4+AgICqj1HI2yJMy7KZDLhwoUL8PPzg0ajuen3ycvLQ1RUFDIyMuDv72/HCul6vNbOw2vtPLzWzsXr7TyOutZCCOTn5yMyMhJabfWzUOp1j4pWq0XTpk3t9n7+/v78Q+8kvNbOw2vtPLzWzsXr7TyOuNY19aSYcTItERERuSwGFSIiInJZDCoAPD098fe//x2enp5ql9Lg8Vo7D6+18/BaOxevt/O4wrWu15NpiYiIqGFjjwoRERG5LAYVIiIiclkMKkREROSyGFSIiIjIZTGoAFi0aBGaN28OLy8v9OjRA/v27VO7pHpt3rx56NatG/z8/NC4cWMMHz4cKSkpVueUlJRg4sSJCA4Ohl6vx4gRI3Dp0iWVKm445s+fD41Gg6lTpyrP8Vrb1/nz5/HXv/4VwcHB8Pb2RocOHbB//37ldSEEXnvtNURERMDb2xv9+/fH6dOnVay4fjIajXj11VcRExMDb29vtGjRAnPmzLG6Nwyv9c3ZvXs3hg4disjISGg0GqxZs8bqdVuu69WrVxEfHw9/f38EBgZi3LhxKCgocEzB4ha3fPlyodPpxGeffSaOHz8unn76aREYGCguXbqkdmn11sCBA8WSJUvEsWPHxOHDh8WQIUNEdHS0KCgoUM4ZP368iIqKEtu2bRP79+8Xd911l+jZs6eKVdd/+/btE82bNxcdO3YUU6ZMUZ7ntbafq1evimbNmokxY8aIvXv3it9//11s3rxZpKamKufMnz9fBAQEiDVr1ogjR46IBx54QMTExIji4mIVK69/5s6dK4KDg8W6detEWlqaWLFihdDr9eK9995TzuG1vjkbNmwQs2bNEqtWrRIAxOrVq61et+W6Dho0SHTq1En88ssv4scffxQtW7YUjz32mEPqveWDSvfu3cXEiROVr41Go4iMjBTz5s1TsaqGJSsrSwAQu3btEkIIkZOTIzw8PMSKFSuUc5KTkwUAsWfPHrXKrNfy8/NFq1atREJCgujTp48SVHit7eull14SvXv3rvJ1k8kkwsPDxYIFC5TncnJyhKenp/jmm2+cUWKDcf/994snn3zS6rkHH3xQxMfHCyF4re3l+qBiy3U9ceKEACCSkpKUczZu3Cg0Go04f/683Wu8pYd+SktLceDAAfTv3195TqvVon///tizZ4+KlTUsubm5AICgoCAAwIEDB1BWVmZ13du0aYPo6Ghe95s0ceJE3H///VbXFOC1trfvv/8eXbt2xcMPP4zGjRujc+fO+OSTT5TX09LScPHiRavrHRAQgB49evB611LPnj2xbds2nDp1CgBw5MgR/PTTTxg8eDAAXmtHseW67tmzB4GBgejatatyTv/+/aHVarF3716711Svb0pYV5cvX4bRaERYWJjV82FhYTh58qRKVTUsJpMJU6dORa9evXD77bcDAC5evAidTofAwECrc8PCwnDx4kUVqqzfli9fjoMHDyIpKemG13it7ev333/H4sWL8fzzz+OVV15BUlISJk+eDJ1Oh9GjRyvXtLK/U3i9a+fll19GXl4e2rRpAzc3NxiNRsydOxfx8fEAwGvtILZc14sXL6Jx48ZWr7u7uyMoKMgh1/6WDirkeBMnTsSxY8fw008/qV1Kg5SRkYEpU6YgISEBXl5eapfT4JlMJnTt2hX/+Mc/AACdO3fGsWPH8J///AejR49WubqG5dtvv8VXX32Fr7/+Gu3bt8fhw4cxdepUREZG8lrfYm7poZ+QkBC4ubndsALi0qVLCA8PV6mqhuO5557DunXrsGPHDjRt2lR5Pjw8HKWlpcjJybE6n9e99g4cOICsrCzceeedcHd3h7u7O3bt2oX3338f7u7uCAsL47W2o4iICLRr187qubZt2+Ls2bMAoFxT/p1SdzNmzMDLL7+MRx99FB06dMATTzyBadOmYd68eQB4rR3FlusaHh6OrKwsq9fLy8tx9epVh1z7Wzqo6HQ6dOnSBdu2bVOeM5lM2LZtG+Li4lSsrH4TQuC5557D6tWrsX37dsTExFi93qVLF3h4eFhd95SUFJw9e5bXvZb69euHo0eP4vDhw0rr2rUr4uPjlWNea/vp1avXDUvtT506hWbNmgEAYmJiEB4ebnW98/LysHfvXl7vWioqKoJWa/0rys3NDSaTCQCvtaPYcl3j4uKQk5ODAwcOKOds374dJpMJPXr0sH9Rdp+eW88sX75ceHp6iqVLl4oTJ06IZ555RgQGBoqLFy+qXVq9NWHCBBEQECB27twpMjMzlVZUVKScM378eBEdHS22b98u9u/fL+Li4kRcXJyKVTccFVf9CMFrbU/79u0T7u7uYu7cueL06dPiq6++Ej4+PuLLL79Uzpk/f74IDAwUa9euFb/++qsYNmwYl8zehNGjR4smTZooy5NXrVolQkJCxIsvvqicw2t9c/Lz88WhQ4fEoUOHBADxzjvviEOHDon09HQhhG3XddCgQaJz585i79694qeffhKtWrXi8mRH+uCDD0R0dLTQ6XSie/fu4pdfflG7pHoNQKVtyZIlyjnFxcXi2WefFY0aNRI+Pj7iL3/5i8jMzFSv6Abk+qDCa21fP/zwg7j99tuFp6enaNOmjfj444+tXjeZTOLVV18VYWFhwtPTU/Tr10+kpKSoVG39lZeXJ6ZMmSKio6OFl5eXuO2228SsWbOEwWBQzuG1vjk7duyo9O/o0aNHCyFsu65XrlwRjz32mNDr9cLf31+MHTtW5OfnO6RejRAVtvkjIiIiciG39BwVIiIicm0MKkREROSyGFSIiIjIZTGoEBERkctiUCEiIiKXxaBCRERELotBhYiIiFwWgwoRERG5LAYVIqrXdu7cCY1Gc8ONF4moYWBQISIiIpfFoEJEREQui0GFiOrEZDJh3rx5iImJgbe3Nzp16oTvvvsOgGVYZv369ejYsSO8vLxw11134dixY1bvsXLlSrRv3x6enp5o3rw5Fi5caPW6wWDASy+9hKioKHh6eqJly5b49NNPrc45cOAAunbtCh8fH/Ts2RMpKSnKa0eOHMG9994LPz8/+Pv7o0uXLti/f7+DrggR2RODChHVybx587Bs2TL85z//wfHjxzFt2jT89a9/xa5du5RzZsyYgYULFyIpKQmhoaEYOnQoysrKAMiAMXLkSDz66KM4evQoZs+ejVdffRVLly5Vvn/UqFH45ptv8P777yM5ORkfffQR9Hq9VR2zZs3CwoULsX//fri7u+PJJ59UXouPj0fTpk2RlJSEAwcO4OWXX4aHh4djLwwR2YdD7slMRLeEkpIS4ePjI37++Wer58eNGycee+wx5Xbyy5cvV167cuWK8Pb2Fv/73/+EEEI8/vjjYsCAAVbfP2PGDNGuXTshhBApKSkCgEhISKi0BvNnbN26VXlu/fr1AoAoLi4WQgjh5+cnli5dWvcfmIicjj0qRHTTUlNTUVRUhAEDBkCv1ytt2bJl+O2335Tz4uLilOOgoCDExsYiOTkZAJCcnIxevXpZvW+vXr1w+vRpGI1GHD58GG5ubujTp0+1tXTs2FE5joiIAABkZWUBAJ5//nk89dRT6N+/P+bPn29VGxG5NgYVIrppBQUFAID169fj8OHDSjtx4oQyT6WuvL29bTqv4lCORqMBIOfPAMDs2bNx/Phx3H///di+fTvatWuH1atX26U+InIsBhUiumnt2rWDp6cnzp49i5YtW1q1qKgo5bxffvlFOc7OzsapU6fQtm1bAEDbtm2RmJho9b6JiYlo3bo13Nzc0KFDB5hMJqs5LzejdevWmDZtGrZs2YIHH3wQS5YsqdP7EZFzuKtdABHVX35+fpg+fTqmTZsGk8mE3r17Izc3F4mJifD390ezZs0AAG+88QaCg4MRFhaGWbNmISQkBMOHDwcAvPDCC+jWrRvmzJmDRx55BHv27MGHH36If//73wCA5s2bY/To0XjyySfx/vvvo1OnTkhPT0dWVhZGjhxZY43FxcWYMWMGHnroIcTExODcuXNISkrCiBEjHHZdiMiO1J4kQ0T1m8lkEu+++66IjY0VHh4eIjQ0VAwcOFDs2rVLmej6ww8/iPbt2wudTie6d+8ujhw5YvUe3333nWjXrp3w8PAQ0dHRYsGCBVavFxcXi2nTpomIiAih0+lEy5YtxWeffSaEsEymzc7OVs4/dOiQACDS0tKEwWAQjz76qIiKihI6nU5ERkaK5557TploS0SuTSOEECpnJSJqoHbu3Il7770X2dnZCAwMVLscIqqHOEeFiIiIXBaDChEREbksDv0QERGRy2KPChEREbksBhUiIiJyWQwqRERE5LIYVIiIiMhlMagQERGRy2JQISIiIpfFoEJEREQui0GFiIiIXNb/A6ZiiQjPfuDYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = [i + 1 for i in range(len(fitdata.history['loss']))]\n",
    "\n",
    "plt.plot(x_axis, fitdata.history['loss'], 'r', label=\"model training loss\")\n",
    "plt.plot(x_axis, fitdata.history['val_loss'], 'b', label=\"validation loss\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"% loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bdab07",
   "metadata": {},
   "source": [
    "# Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "704ed038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'golden_retriever'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = tf.keras.utils.load_img(\"./goldret.jpg\")\n",
    "input_arr = tf.keras.utils.img_to_array(image)\n",
    "input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "\n",
    "image_ds.class_names[np.argmax(classifier.predict(input_arr))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
